[DQN]
### experiment ###
GPU_INDEX = 2
PRE_TRAIN_MODEL_FILE
EPOCH_NUM = 360
EPOCH_LENGTH = 30000

### game env ###
GAME_NAME = medusa
ACTION_NUM = 5
OBSERVATION_TYPE = gray
CHANNEL = 4
WIDTH = 200
HEIGHT = 100

### player ###
TRAIN_PER_STEP = 4

### replay buffer ###
PHI_LENGTH = 1
BUFFER_MAX = 10000
BEGIN_RANDOM_STEP = 300


### q-learning ###
DISCOUNT = 0.90
EPSILON_MIN = 0.15
EPSILON_START = 1.0
EPSILON_DECAY = 500000


UPDATE_TARGET_BY_EPISODE_END = 50
UPDATE_TARGET_BY_EPISODE_BEGIN = 5
# update UPDATE_TARGET_DECAY times to get to UPDATE_TARGET_BY_EPISODE_END
UPDATE_TARGET_DECAY = 200

OPTIMIZER = adagrad
LEARNING_RATE = 0.005
WEIGHT_DECAY = 0.0
GRAD_CLIPPING_THETA = 0.01

POSITIVE_REWARD = 0.1
NEGATIVE_REWARD = -1

### OTHER ###
;MODEL_PATH = /Users/sky/PycharmProjects/esheep-sdk/model
MODEL_PATH = /home/caoshenkai/medusa/esheep-sdk/model
MODEL_FILE_MARK = dqn_model_1001

EDITED_TIME = 2018-9-26 10:47:06